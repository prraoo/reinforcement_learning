{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gridworld import GridWorld\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import irl.linear_irl as linear_irl\n",
    "import irl.mdp.gridworld as gridworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_policy(policy_matrix):\n",
    "    '''Print the policy using specific symbol.\n",
    "\n",
    "    * terminal state\n",
    "    ^ > v < up, right, down, left\n",
    "    # obstacle\n",
    "    '''\n",
    "    counter = 0\n",
    "    shape = policy_matrix.shape\n",
    "    policy_string = \"\"\n",
    "    for row in range(shape[0]):\n",
    "        for col in range(shape[1]):\n",
    "            if(policy_matrix[row,col] == -1): policy_string += \" *  \"\n",
    "            elif(policy_matrix[row,col] == 0): policy_string += \" ^  \"\n",
    "            elif(policy_matrix[row,col] == 1): policy_string += \" >  \"\n",
    "            elif(policy_matrix[row,col] == 2): policy_string += \" v  \"\n",
    "            elif(policy_matrix[row,col] == 3): policy_string += \" <  \"\n",
    "            elif(np.isnan(policy_matrix[row,col])): policy_string += \" #  \"\n",
    "            counter += 1\n",
    "        policy_string += '\\n'\n",
    "    print(policy_string)\n",
    "\n",
    "def get_return(state_list, gamma):\n",
    "    '''Get the return for a list of action-state values.\n",
    "\n",
    "    @return get the Return\n",
    "    '''\n",
    "    counter = 0\n",
    "    return_value = 0\n",
    "    for visit in state_list:\n",
    "        reward = visit[2]\n",
    "        return_value += reward * np.power(gamma, counter)\n",
    "        counter += 1\n",
    "    return return_value\n",
    "\n",
    "def update_policy(episode_list, policy_matrix, state_action_matrix):\n",
    "    '''Update a policy making it greedy in respect of the state-action matrix.\n",
    "\n",
    "    @return the updated policy\n",
    "    '''\n",
    "    for visit in episode_list:\n",
    "        observation = visit[0]\n",
    "        col = observation[1] + (observation[0]*4)\n",
    "        if(policy_matrix[observation[0], observation[1]] != -1):\n",
    "            policy_matrix[observation[0], observation[1]] = \\\n",
    "                np.argmax(state_action_matrix[:,col])\n",
    "    return policy_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Matrix:\n",
      "[[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "Reward Matrix:\n",
      "[[0 0 0 0 1]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n",
      "('------>', 0)\n",
      "State-Action matrix after 1 iterations:\n",
      "[[9.04915711e+09 9.14203588e+09 5.98113494e+09 4.51999721e+09\n",
      "  5.66044743e+08 4.57367081e+09 4.95383213e+09 8.44101313e+09\n",
      "  1.51467132e+09 9.25609627e+09 5.40014766e+09 3.56257496e+09\n",
      "  6.95767866e+09 8.64774793e+09 5.49980032e+09 7.52974974e+09\n",
      "  1.57975385e-03 2.46520549e+09 6.51065756e+09 4.39105621e+09\n",
      "  1.95427167e+09 8.89142644e+09 6.00379767e+09 8.17092020e+09\n",
      "  6.87226173e+09]\n",
      " [2.17815216e+09 5.39106403e+08 5.57379179e+09 4.19195681e+08\n",
      "  1.79086699e+09 4.29874164e+09 2.06881745e+09 3.09632514e+09\n",
      "  1.53235871e+09 1.49048905e+09 5.54435287e+09 2.52223182e+09\n",
      "  9.69278114e+08 8.26856548e+09 3.31934266e+09 4.30623501e+09\n",
      "  5.12712210e-04 4.08136625e+09 6.25517450e+09 7.44378437e+09\n",
      "  3.56653375e+09 3.18238321e+09 7.85115510e+09 5.08855972e+09\n",
      "  7.53051314e+09]\n",
      " [4.01408164e+08 4.63274944e+09 8.38985508e+09 3.50439033e+09\n",
      "  4.22529725e+09 9.59793557e+09 5.32770804e+09 7.32914127e+09\n",
      "  1.02555309e+09 9.60688700e+08 8.12717381e+09 5.72913282e+08\n",
      "  5.26911071e+09 9.68387187e+08 3.47426570e+09 5.70735402e+09\n",
      "  1.40073486e-03 3.42015772e+09 9.32841276e+09 3.21267946e+09\n",
      "  3.73918129e+09 3.71917311e+09 5.82304864e+09 3.31802116e+09\n",
      "  5.87270143e+09]\n",
      " [8.82264516e+09 6.95447129e+09 8.39107204e+08 5.88784592e+09\n",
      "  5.57194870e+09 9.83327747e+09 8.54345055e+09 6.57642304e+09\n",
      "  1.33357130e+09 1.65313171e+09 8.70324017e+09 9.24118728e+09\n",
      "  4.52415374e+09 6.12136122e+09 8.83296593e+09 6.19691095e+09\n",
      "  1.68714014e-03 5.78627009e+09 8.81722953e+09 3.82846594e+09\n",
      "  9.35404013e+09 2.09680299e+09 5.60592718e+08 1.91823732e+09\n",
      "  3.32496866e+09]]\n",
      "Policy matrix after 1 iterations:\n",
      "[[ 0.  1.  1.  2. -1.]\n",
      " [ 3.  1.  3.  1.  3.]\n",
      " [ 0.  0.  0.  0.  2.]\n",
      " [ 2.  2.  1.  3.  0.]\n",
      " [ 3.  0.  2.  0.  0.]]\n",
      "Policy Directions\n",
      " ^   >   >   v   *  \n",
      " <   >   <   >   <  \n",
      " ^   ^   ^   ^   v  \n",
      " v   v   >   <   ^  \n",
      " <   ^   v   ^   ^  \n",
      "\n",
      "('------>', 0)\n",
      "State-Action matrix after 6 iterations:\n",
      "[[9.04915711e+09 9.14203588e+09 5.98113494e+09 4.51999721e+09\n",
      "  5.66044743e+08 4.57367081e+09 4.95383213e+09 8.44101313e+09\n",
      "  1.51467132e+09 9.25609627e+09 5.40014766e+09 3.56257496e+09\n",
      "  6.95767866e+09 8.64774793e+09 5.49980032e+09 7.52974974e+09\n",
      "  1.90796519e-04 2.46520549e+09 6.51065756e+09 4.39105621e+09\n",
      "  1.95427167e+09 8.89142644e+09 6.00379767e+09 8.17092020e+09\n",
      "  6.87226173e+09]\n",
      " [2.17815216e+09 5.39106403e+08 5.57379179e+09 4.19195681e+08\n",
      "  1.79086699e+09 4.29874164e+09 2.06881745e+09 3.09632514e+09\n",
      "  1.53235871e+09 1.49048905e+09 5.54435287e+09 2.52223182e+09\n",
      "  9.69278114e+08 8.26856548e+09 3.31934266e+09 4.30623501e+09\n",
      "  1.98930906e-04 4.08136625e+09 6.25517450e+09 7.44378437e+09\n",
      "  3.56653375e+09 3.18238321e+09 7.85115510e+09 5.08855972e+09\n",
      "  7.53051314e+09]\n",
      " [4.01408164e+08 4.63274944e+09 8.38985508e+09 3.50439033e+09\n",
      "  4.22529725e+09 9.59793557e+09 5.32770804e+09 7.32914127e+09\n",
      "  1.02555309e+09 9.60688700e+08 8.12717381e+09 5.72913282e+08\n",
      "  5.26911071e+09 9.68387187e+08 3.47426570e+09 5.70735402e+09\n",
      "  1.91734252e-04 3.42015772e+09 9.32841276e+09 3.21267946e+09\n",
      "  3.73918129e+09 3.71917311e+09 5.82304864e+09 3.31802116e+09\n",
      "  5.87270143e+09]\n",
      " [8.82264516e+09 6.95447129e+09 8.39107204e+08 5.88784592e+09\n",
      "  5.57194870e+09 9.83327747e+09 8.54345055e+09 6.57642304e+09\n",
      "  1.33357130e+09 1.65313171e+09 8.70324017e+09 9.24118728e+09\n",
      "  4.52415374e+09 6.12136122e+09 8.83296593e+09 6.19691095e+09\n",
      "  1.99141382e-04 5.78627009e+09 8.81722953e+09 3.82846594e+09\n",
      "  9.35404013e+09 2.09680299e+09 5.60592718e+08 1.91823732e+09\n",
      "  3.32496866e+09]]\n",
      "Policy matrix after 6 iterations:\n",
      "[[ 0.  1.  1.  2. -1.]\n",
      " [ 3.  1.  3.  1.  3.]\n",
      " [ 0.  0.  0.  0.  2.]\n",
      " [ 2.  2.  1.  3.  0.]\n",
      " [ 3.  0.  2.  0.  0.]]\n",
      "Policy Directions\n",
      " ^   >   >   v   *  \n",
      " <   >   <   >   <  \n",
      " ^   ^   ^   ^   v  \n",
      " v   v   >   <   ^  \n",
      " <   ^   v   ^   ^  \n",
      "\n",
      "('------>', 0)\n",
      "State-Action matrix after 11 iterations:\n",
      "[[9.04915711e+09 9.14203588e+09 5.98113494e+09 4.51999721e+09\n",
      "  5.66044743e+08 4.57367081e+09 4.95383213e+09 8.44101313e+09\n",
      "  1.51467132e+09 9.25609627e+09 5.40014766e+09 3.56257496e+09\n",
      "  6.95767866e+09 8.64774793e+09 5.49980032e+09 7.52974974e+09\n",
      "  9.58790134e-05 2.46520549e+09 6.51065756e+09 4.39105621e+09\n",
      "  1.95427167e+09 8.89142644e+09 6.00379767e+09 8.17092020e+09\n",
      "  6.87226173e+09]\n",
      " [2.17815216e+09 5.39106403e+08 5.57379179e+09 4.19195681e+08\n",
      "  1.79086699e+09 4.29874164e+09 2.06881745e+09 3.09632514e+09\n",
      "  1.53235871e+09 1.49048905e+09 5.54435287e+09 2.52223182e+09\n",
      "  9.69278114e+08 8.26856548e+09 3.31934266e+09 4.30623501e+09\n",
      "  9.03830021e-05 4.08136625e+09 6.25517450e+09 7.44378437e+09\n",
      "  3.56653375e+09 3.18238321e+09 7.85115510e+09 5.08855972e+09\n",
      "  7.53051314e+09]\n",
      " [4.01408164e+08 4.63274944e+09 8.38985508e+09 3.50439033e+09\n",
      "  4.22529725e+09 9.59793557e+09 5.32770804e+09 7.32914127e+09\n",
      "  1.02555309e+09 9.60688700e+08 8.12717381e+09 5.72913282e+08\n",
      "  5.26911071e+09 9.68387187e+08 3.47426570e+09 5.70735402e+09\n",
      "  9.48575038e-05 3.42015772e+09 9.32841276e+09 3.21267946e+09\n",
      "  3.73918129e+09 3.71917311e+09 5.82304864e+09 3.31802116e+09\n",
      "  5.87270143e+09]\n",
      " [8.82264516e+09 6.95447129e+09 8.39107204e+08 5.88784592e+09\n",
      "  5.57194870e+09 9.83327747e+09 8.54345055e+09 6.57642304e+09\n",
      "  1.33357130e+09 1.65313171e+09 8.70324017e+09 9.24118728e+09\n",
      "  4.52415374e+09 6.12136122e+09 8.83296593e+09 6.19691095e+09\n",
      "  9.65421978e-05 5.78627009e+09 8.81722953e+09 3.82846594e+09\n",
      "  9.35404013e+09 2.09680299e+09 5.60592718e+08 1.91823732e+09\n",
      "  3.32496866e+09]]\n",
      "Policy matrix after 11 iterations:\n",
      "[[ 0.  1.  1.  2. -1.]\n",
      " [ 3.  1.  3.  1.  3.]\n",
      " [ 0.  0.  0.  0.  2.]\n",
      " [ 2.  2.  1.  3.  0.]\n",
      " [ 3.  0.  2.  0.  0.]]\n",
      "Policy Directions\n",
      " ^   >   >   v   *  \n",
      " <   >   <   >   <  \n",
      " ^   ^   ^   ^   v  \n",
      " v   v   >   <   ^  \n",
      " <   ^   v   ^   ^  \n",
      "\n",
      "('------>', 0)\n",
      "State-Action matrix after 16 iterations:\n",
      "[[9.04915711e+09 9.14203588e+09 5.98113494e+09 4.51999721e+09\n",
      "  5.66044743e+08 4.57367081e+09 4.95383213e+09 8.44101313e+09\n",
      "  1.51467132e+09 9.25609627e+09 5.40014766e+09 3.56257496e+09\n",
      "  6.95767866e+09 8.64774793e+09 5.49980032e+09 7.52974974e+09\n",
      "  6.76496587e-05 2.46520549e+09 6.51065756e+09 4.39105621e+09\n",
      "  1.95427167e+09 8.89142644e+09 6.00379767e+09 8.17092020e+09\n",
      "  6.87226173e+09]\n",
      " [2.17815216e+09 5.39106403e+08 5.57379179e+09 4.19195681e+08\n",
      "  1.79086699e+09 4.29874164e+09 2.06881745e+09 3.09632514e+09\n",
      "  1.53235871e+09 1.49048905e+09 5.54435287e+09 2.52223182e+09\n",
      "  9.69278114e+08 8.26856548e+09 3.31934266e+09 4.30623501e+09\n",
      "  6.45826889e-05 4.08136625e+09 6.25517450e+09 7.44378437e+09\n",
      "  3.56653375e+09 3.18238321e+09 7.85115510e+09 5.08855972e+09\n",
      "  7.53051314e+09]\n",
      " [4.01408164e+08 4.63274944e+09 8.38985508e+09 3.50439033e+09\n",
      "  4.22529725e+09 9.59793557e+09 5.32770804e+09 7.32914127e+09\n",
      "  1.02555309e+09 9.60688700e+08 8.12717381e+09 5.72913282e+08\n",
      "  5.26911071e+09 9.68387187e+08 3.47426570e+09 5.70735402e+09\n",
      "  6.45882078e-05 3.42015772e+09 9.32841276e+09 3.21267946e+09\n",
      "  3.73918129e+09 3.71917311e+09 5.82304864e+09 3.31802116e+09\n",
      "  5.87270143e+09]\n",
      " [8.82264516e+09 6.95447129e+09 8.39107204e+08 5.88784592e+09\n",
      "  5.57194870e+09 9.83327747e+09 8.54345055e+09 6.57642304e+09\n",
      "  1.33357130e+09 1.65313171e+09 8.70324017e+09 9.24118728e+09\n",
      "  4.52415374e+09 6.12136122e+09 8.83296593e+09 6.19691095e+09\n",
      "  6.79656512e-05 5.78627009e+09 8.81722953e+09 3.82846594e+09\n",
      "  9.35404013e+09 2.09680299e+09 5.60592718e+08 1.91823732e+09\n",
      "  3.32496866e+09]]\n",
      "Policy matrix after 16 iterations:\n",
      "[[ 0.  1.  1.  2. -1.]\n",
      " [ 3.  1.  3.  1.  3.]\n",
      " [ 0.  0.  0.  0.  2.]\n",
      " [ 2.  2.  1.  3.  0.]\n",
      " [ 3.  0.  2.  0.  0.]]\n",
      "Policy Directions\n",
      " ^   >   >   v   *  \n",
      " <   >   <   >   <  \n",
      " ^   ^   ^   ^   v  \n",
      " v   v   >   <   ^  \n",
      " <   ^   v   ^   ^  \n",
      "\n",
      "('------>', 0)\n",
      "State-Action matrix after 21 iterations:\n",
      "[[9.04915711e+09 9.14203588e+09 5.98113494e+09 4.51999721e+09\n",
      "  5.66044743e+08 4.57367081e+09 4.95383213e+09 8.44101313e+09\n",
      "  1.51467132e+09 9.25609627e+09 5.40014766e+09 3.56257496e+09\n",
      "  6.95767866e+09 8.64774793e+09 5.49980032e+09 7.52974974e+09\n",
      "  4.97065732e-05 2.46520549e+09 6.51065756e+09 4.39105621e+09\n",
      "  1.95427167e+09 8.89142644e+09 6.00379767e+09 8.17092020e+09\n",
      "  6.87226173e+09]\n",
      " [2.17815216e+09 5.39106403e+08 5.57379179e+09 4.19195681e+08\n",
      "  1.79086699e+09 4.29874164e+09 2.06881745e+09 3.09632514e+09\n",
      "  1.53235871e+09 1.49048905e+09 5.54435287e+09 2.52223182e+09\n",
      "  9.69278114e+08 8.26856548e+09 3.31934266e+09 4.30623501e+09\n",
      "  4.03836221e-05 4.08136625e+09 6.25517450e+09 7.44378437e+09\n",
      "  3.56653375e+09 3.18238321e+09 7.85115510e+09 5.08855972e+09\n",
      "  7.53051314e+09]\n",
      " [4.01408164e+08 4.63274944e+09 8.38985508e+09 3.50439033e+09\n",
      "  4.22529725e+09 9.59793557e+09 5.32770804e+09 7.32914127e+09\n",
      "  1.02555309e+09 9.60688700e+08 8.12717381e+09 5.72913282e+08\n",
      "  5.26911071e+09 9.68387187e+08 3.47426570e+09 5.70735402e+09\n",
      "  4.94261343e-05 3.42015772e+09 9.32841276e+09 3.21267946e+09\n",
      "  3.73918129e+09 3.71917311e+09 5.82304864e+09 3.31802116e+09\n",
      "  5.87270143e+09]\n",
      " [8.82264516e+09 6.95447129e+09 8.39107204e+08 5.88784592e+09\n",
      "  5.57194870e+09 9.83327747e+09 8.54345055e+09 6.57642304e+09\n",
      "  1.33357130e+09 1.65313171e+09 8.70324017e+09 9.24118728e+09\n",
      "  4.52415374e+09 6.12136122e+09 8.83296593e+09 6.19691095e+09\n",
      "  4.98781642e-05 5.78627009e+09 8.81722953e+09 3.82846594e+09\n",
      "  9.35404013e+09 2.09680299e+09 5.60592718e+08 1.91823732e+09\n",
      "  3.32496866e+09]]\n",
      "Policy matrix after 21 iterations:\n",
      "[[ 0.  1.  1.  2. -1.]\n",
      " [ 3.  1.  3.  1.  3.]\n",
      " [ 0.  0.  0.  0.  2.]\n",
      " [ 2.  2.  1.  3.  0.]\n",
      " [ 3.  0.  2.  0.  0.]]\n",
      "Policy Directions\n",
      " ^   >   >   v   *  \n",
      " <   >   <   >   <  \n",
      " ^   ^   ^   ^   v  \n",
      " v   v   >   <   ^  \n",
      " <   ^   v   ^   ^  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('------>', 0)\n",
      "State-Action matrix after 26 iterations:\n",
      "[[9.04915711e+09 9.14203588e+09 5.98113494e+09 4.51999721e+09\n",
      "  5.66044743e+08 4.57367081e+09 4.95383213e+09 8.44101313e+09\n",
      "  1.51467132e+09 9.25609627e+09 5.40014766e+09 3.56257496e+09\n",
      "  6.95767866e+09 8.64774793e+09 5.49980032e+09 7.52974974e+09\n",
      "  3.92934779e-05 2.46520549e+09 6.51065756e+09 4.39105621e+09\n",
      "  1.95427167e+09 8.89142644e+09 6.00379767e+09 8.17092020e+09\n",
      "  6.87226173e+09]\n",
      " [2.17815216e+09 5.39106403e+08 5.57379179e+09 4.19195681e+08\n",
      "  1.79086699e+09 4.29874164e+09 2.06881745e+09 3.09632514e+09\n",
      "  1.53235871e+09 1.49048905e+09 5.54435287e+09 2.52223182e+09\n",
      "  9.69278114e+08 8.26856548e+09 3.31934266e+09 4.30623501e+09\n",
      "  3.73027988e-05 4.08136625e+09 6.25517450e+09 7.44378437e+09\n",
      "  3.56653375e+09 3.18238321e+09 7.85115510e+09 5.08855972e+09\n",
      "  7.53051314e+09]\n",
      " [4.01408164e+08 4.63274944e+09 8.38985508e+09 3.50439033e+09\n",
      "  4.22529725e+09 9.59793557e+09 5.32770804e+09 7.32914127e+09\n",
      "  1.02555309e+09 9.60688700e+08 8.12717381e+09 5.72913282e+08\n",
      "  5.26911071e+09 9.68387187e+08 3.47426570e+09 5.70735402e+09\n",
      "  3.90800856e-05 3.42015772e+09 9.32841276e+09 3.21267946e+09\n",
      "  3.73918129e+09 3.71917311e+09 5.82304864e+09 3.31802116e+09\n",
      "  5.87270143e+09]\n",
      " [8.82264516e+09 6.95447129e+09 8.39107204e+08 5.88784592e+09\n",
      "  5.57194870e+09 9.83327747e+09 8.54345055e+09 6.57642304e+09\n",
      "  1.33357130e+09 1.65313171e+09 8.70324017e+09 9.24118728e+09\n",
      "  4.52415374e+09 6.12136122e+09 8.83296593e+09 6.19691095e+09\n",
      "  3.93926284e-05 5.78627009e+09 8.81722953e+09 3.82846594e+09\n",
      "  9.35404013e+09 2.09680299e+09 5.60592718e+08 1.91823732e+09\n",
      "  3.32496866e+09]]\n",
      "Policy matrix after 26 iterations:\n",
      "[[ 0.  1.  1.  2. -1.]\n",
      " [ 3.  1.  3.  1.  3.]\n",
      " [ 0.  0.  0.  0.  2.]\n",
      " [ 2.  2.  1.  3.  0.]\n",
      " [ 3.  0.  2.  0.  0.]]\n",
      "Policy Directions\n",
      " ^   >   >   v   *  \n",
      " <   >   <   >   <  \n",
      " ^   ^   ^   ^   v  \n",
      " v   v   >   <   ^  \n",
      " <   ^   v   ^   ^  \n",
      "\n",
      "Utility matrix after 30 iterations:\n",
      "[[9.04915711e+09 9.14203588e+09 5.98113494e+09 4.51999721e+09\n",
      "  5.66044743e+08 4.57367081e+09 4.95383213e+09 8.44101313e+09\n",
      "  1.51467132e+09 9.25609627e+09 5.40014766e+09 3.56257496e+09\n",
      "  6.95767866e+09 8.64774793e+09 5.49980032e+09 7.52974974e+09\n",
      "  3.36427937e-05 2.46520549e+09 6.51065756e+09 4.39105621e+09\n",
      "  1.95427167e+09 8.89142644e+09 6.00379767e+09 8.17092020e+09\n",
      "  6.87226173e+09]\n",
      " [2.17815216e+09 5.39106403e+08 5.57379179e+09 4.19195681e+08\n",
      "  1.79086699e+09 4.29874164e+09 2.06881745e+09 3.09632514e+09\n",
      "  1.53235871e+09 1.49048905e+09 5.54435287e+09 2.52223182e+09\n",
      "  9.69278114e+08 8.26856548e+09 3.31934266e+09 4.30623501e+09\n",
      "  2.95652342e-05 4.08136625e+09 6.25517450e+09 7.44378437e+09\n",
      "  3.56653375e+09 3.18238321e+09 7.85115510e+09 5.08855972e+09\n",
      "  7.53051314e+09]\n",
      " [4.01408164e+08 4.63274944e+09 8.38985508e+09 3.50439033e+09\n",
      "  4.22529725e+09 9.59793557e+09 5.32770804e+09 7.32914127e+09\n",
      "  1.02555309e+09 9.60688700e+08 8.12717381e+09 5.72913282e+08\n",
      "  5.26911071e+09 9.68387187e+08 3.47426570e+09 5.70735402e+09\n",
      "  3.35124330e-05 3.42015772e+09 9.32841276e+09 3.21267946e+09\n",
      "  3.73918129e+09 3.71917311e+09 5.82304864e+09 3.31802116e+09\n",
      "  5.87270143e+09]\n",
      " [8.82264516e+09 6.95447129e+09 8.39107204e+08 5.88784592e+09\n",
      "  5.57194870e+09 9.83327747e+09 8.54345055e+09 6.57642304e+09\n",
      "  1.33357130e+09 1.65313171e+09 8.70324017e+09 9.24118728e+09\n",
      "  4.52415374e+09 6.12136122e+09 8.83296593e+09 6.19691095e+09\n",
      "  3.37209297e-05 5.78627009e+09 8.81722953e+09 3.82846594e+09\n",
      "  9.35404013e+09 2.09680299e+09 5.60592718e+08 1.91823732e+09\n",
      "  3.32496866e+09]]\n",
      "-------END--------\n"
     ]
    }
   ],
   "source": [
    "env = GridWorld(5, 5)\n",
    "\n",
    "#Define the state matrix\n",
    "state_matrix = np.zeros((5,5))\n",
    "state_matrix[0, 4] = 1\n",
    "print(\"State Matrix:\")\n",
    "print(state_matrix)\n",
    "\n",
    "#Define the reward matrix\n",
    "reward_matrix = np.full((5,5), 0)\n",
    "reward_matrix[0, 4] = 1\n",
    "print(\"Reward Matrix:\")\n",
    "print(reward_matrix)\n",
    "\n",
    "#Define the transition matrix\n",
    "transition_matrix = np.array([[0.7, 0.1, 0.1, 0.1],\n",
    "                              [0.1, 0.7, 0.1, 0.1],\n",
    "                              [0.1, 0.1, 0.7, 0.1],\n",
    "                              [0.1, 0.1, 0.1, 0.7]])\n",
    "\n",
    "#Random policy\n",
    "policy_matrix = np.random.randint(low=0, high=4, size=(5, 5)).astype(np.float32)\n",
    "policy_matrix[0,4] = -1\n",
    "\n",
    "#Set the matrices in the world\n",
    "env.setStateMatrix(state_matrix)\n",
    "env.setRewardMatrix(reward_matrix)\n",
    "env.setTransitionMatrix(transition_matrix)\n",
    "\n",
    "state_action_matrix = np.random.random_sample((4,25)) # Q\n",
    "#init with 1.0e-10 to avoid division by zero\n",
    "running_mean_matrix = np.full((4,25), 1.0e-10)\n",
    "gamma = 0.999\n",
    "tot_epoch = 30\n",
    "print_epoch = 5\n",
    "\n",
    "for epoch in range(tot_epoch):\n",
    "    #Starting a new episode\n",
    "    episode_list = list()\n",
    "    #Reset and return the first observation and reward\n",
    "    observation = env.reset(exploring_starts=False)\n",
    "\n",
    "    is_starting = True\n",
    "    for _ in range(1000):\n",
    "        #Take the action from the action matrix\n",
    "        action = policy_matrix[observation[0], observation[1]]\n",
    "        #If the episode just started then it is\n",
    "            #necessary to choose a random action (exploring starts)\n",
    "        if(is_starting):\n",
    "            action = np.random.randint(0, 4)\n",
    "            is_starting = False\n",
    "        #Move one step in the environment and get obs and reward\n",
    "        new_observation, reward, done = env.step(action)\n",
    "        #Append the visit in the episode list\n",
    "        episode_list.append((observation, action, reward))\n",
    "        observation = new_observation\n",
    "        \n",
    "        if done: break\n",
    "            \n",
    "        #The episode is finished, now estimating the utilities\n",
    "        counter = 0\n",
    "        #Checkup to identify if it is the first visit to a state\n",
    "        checkup_matrix = np.zeros((4,25))\n",
    "        #This cycle is the implementation of First-Visit MC.\n",
    "\n",
    "        for visit in episode_list:\n",
    "            observation = visit[0]\n",
    "            action = visit[1]\n",
    "            col = int(observation[1] + (observation[0]*4))\n",
    "            row = int(action)\n",
    "            if(checkup_matrix[row, col] == 0):\n",
    "                return_value = get_return(episode_list[counter:], gamma)\n",
    "                running_mean_matrix[row, col] += 1\n",
    "                state_action_matrix[row, col] += return_value\n",
    "                checkup_matrix[row, col] = 1\n",
    "            counter += 1\n",
    "        #Policy Update\n",
    "        policy_matrix = update_policy(episode_list,\n",
    "                                      policy_matrix,\n",
    "                                      state_action_matrix/running_mean_matrix)\n",
    "\n",
    "    if(epoch % print_epoch == 0):\n",
    "        print(\"------>\",epoch % print_epoch)\n",
    "        print(\"State-Action matrix after \" + str(epoch+1) + \" iterations:\")\n",
    "        print(state_action_matrix / running_mean_matrix)\n",
    "        print(\"Policy matrix after \" + str(epoch+1) + \" iterations:\")\n",
    "        print(policy_matrix)\n",
    "        print(\"Policy Directions\")\n",
    "        print_policy(policy_matrix)\n",
    "\n",
    "\n",
    "#Time to check the utility matrix obtained\n",
    "print(\"Utility matrix after \" + str(tot_epoch) + \" iterations:\")\n",
    "print(state_action_matrix / running_mean_matrix)\n",
    "        \n",
    "print(\"-------END--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.90491571, 0.91420359, 0.83898551, 0.58878459, 0.55719487,\n",
       "       0.98332775, 0.85434505, 0.84410131, 0.15323587, 0.92560963,\n",
       "       0.87032402, 0.92411873, 0.69576787, 0.86477479, 0.88329659,\n",
       "       0.75297497, 0.93636278, 0.57862701, 0.93284128, 0.74437844,\n",
       "       0.93540401, 0.88914264, 0.78511551, 0.81709202, 0.75305131])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_value_matrix = state_action_matrix.max(axis=0)\n",
    "state_value_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvxopt import matrix, solvers\n",
    "\n",
    "def irl(n_states, n_actions, transition_probability, policy, discount, Rmax,\n",
    "        l1):\n",
    "    \"\"\"\n",
    "    Find a reward function with inverse RL as described in Ng & Russell, 2000.\n",
    "\n",
    "    n_states: Number of states. int.\n",
    "    n_actions: Number of actions. int.\n",
    "    transition_probability: NumPy array mapping (state_i, action, state_k) to\n",
    "        the probability of transitioning from state_i to state_k under action.\n",
    "        Shape (N, A, N).\n",
    "    policy: Vector mapping state ints to action ints. Shape (N,).\n",
    "    discount: Discount factor. float.\n",
    "    Rmax: Maximum reward. float.\n",
    "    l1: l1 regularisation. float.\n",
    "    -> Reward vector\n",
    "    \"\"\"\n",
    "\n",
    "    A = set(range(n_actions))  # Set of actions to help manage reordering\n",
    "                               # actions.\n",
    "    # The transition policy convention is different here to the rest of the code\n",
    "    # for legacy reasons; here, we reorder axes to fix this. We expect the\n",
    "    # new probabilities to be of the shape (A, N, N).\n",
    "    transition_probability = np.transpose(transition_probability, (1, 0, 2))\n",
    "\n",
    "    def T(a, s):\n",
    "        \"\"\"\n",
    "        Shorthand for a dot product used a lot in the LP formulation.\n",
    "        \"\"\"\n",
    "\n",
    "        return np.dot(transition_probability[policy[s], s] -\n",
    "                      transition_probability[a, s],\n",
    "                      np.linalg.inv(np.eye(n_states) -\n",
    "                        discount*transition_probability[policy[s]]))\n",
    "\n",
    "    # This entire function just computes the block matrices used for the LP\n",
    "    # formulation of IRL.\n",
    "\n",
    "    # Minimise c . x.\n",
    "    c = -np.hstack([np.zeros(n_states), np.ones(n_states),\n",
    "                    -l1*np.ones(n_states)])\n",
    "    zero_stack1 = np.zeros((n_states*(n_actions-1), n_states))\n",
    "    T_stack = np.vstack([\n",
    "        -T(a, s)\n",
    "        for s in range(n_states)\n",
    "        for a in A - {policy[s]}\n",
    "    ])\n",
    "    I_stack1 = np.vstack([\n",
    "        np.eye(1, n_states, s)\n",
    "        for s in range(n_states)\n",
    "        for a in A - {policy[s]}\n",
    "    ])\n",
    "    I_stack2 = np.eye(n_states)\n",
    "    zero_stack2 = np.zeros((n_states, n_states))\n",
    "\n",
    "    D_left = np.vstack([T_stack, T_stack, -I_stack2, I_stack2])\n",
    "    D_middle = np.vstack([I_stack1, zero_stack1, zero_stack2, zero_stack2])\n",
    "    D_right = np.vstack([zero_stack1, zero_stack1, -I_stack2, -I_stack2])\n",
    "\n",
    "    D = np.hstack([D_left, D_middle, D_right])\n",
    "    b = np.zeros((n_states*(n_actions-1)*2 + 2*n_states, 1))\n",
    "    bounds = np.array([(None, None)]*2*n_states + [(-Rmax, Rmax)]*n_states)\n",
    "\n",
    "    # We still need to bound R. To do this, we just add\n",
    "    # -I R <= Rmax 1\n",
    "    # I R <= Rmax 1\n",
    "    # So to D we need to add -I and I, and to b we need to add Rmax 1 and Rmax 1\n",
    "    D_bounds = np.hstack([\n",
    "        np.vstack([\n",
    "            -np.eye(n_states),\n",
    "            np.eye(n_states)]),\n",
    "        np.vstack([\n",
    "            np.zeros((n_states, n_states)),\n",
    "            np.zeros((n_states, n_states))]),\n",
    "        np.vstack([\n",
    "            np.zeros((n_states, n_states)),\n",
    "            np.zeros((n_states, n_states))])])\n",
    "    b_bounds = np.vstack([Rmax*np.ones((n_states, 1))]*2)\n",
    "    D = np.vstack((D, D_bounds))\n",
    "    b = np.vstack((b, b_bounds))\n",
    "    A_ub = matrix(D)\n",
    "    b = matrix(b)\n",
    "    c = matrix(c)\n",
    "    results = solvers.lp(c, A_ub, b)\n",
    "    r = np.asarray(results[\"x\"][:n_states], dtype=np.double)\n",
    "\n",
    "    return r.reshape((n_states,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random State Transition Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 25)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_state_transition_matrix = np.random.rand(25,25)\n",
    "random_state_transition_matrix = random_state_transition_matrix/ \\\n",
    "                                random_state_transition_matrix.sum(axis=1)[:,None]\n",
    "random_state_transition_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With a handcrafted State Transition Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 5\n",
    "discount = 0.2\n",
    "\"\"\"\n",
    "Run linear programming inverse reinforcement learning on the gridworld MDP.\n",
    "\n",
    "Plots the reward function.\n",
    "\n",
    "grid_size: Grid size. int.\n",
    "discount: MDP discount factor. float.\n",
    "\"\"\"\n",
    "\n",
    "wind = 0.3\n",
    "trajectory_length = 3*grid_size\n",
    "\n",
    "gw = gridworld.Gridworld(grid_size, wind, discount)\n",
    "\n",
    "ground_r = np.array([gw.reward(s) for s in range(gw.n_states)])\n",
    "\n",
    "#policy = [gw.optimal_policy_deterministic(s) for s in range(gw.n_states)]\n",
    "r = linear_irl.irl(gw.n_states, gw.n_actions, gw.transition_probability,\n",
    "        policy, gw.discount, 1, 5)\n",
    "\n",
    "print(r.shape)\n",
    "print(gw.optimal_policy_deterministic)\n",
    "print(np.array(policy).reshape(5,5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pcolor(ground_r.reshape((grid_size, grid_size)))\n",
    "plt.colorbar()\n",
    "plt.title(\"Groundtruth reward\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pcolor(r.reshape((grid_size, grid_size)))\n",
    "plt.colorbar()\n",
    "plt.title(\"Recovered reward\")\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
