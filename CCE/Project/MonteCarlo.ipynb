{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Monte Carlo method\n",
    "\n",
    "MC allow learning optimal behaviour directly from interaction with the environment.\n",
    "\n",
    "MC methods is useful when:\n",
    "1. Unknown transition model \n",
    "2. Reward function is unknown\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gridworld import GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The world has 3 rows and 4 columns\n",
    "env = GridWorld(5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gridworld import GridWorld\n",
    "\n",
    "# Declare our environmnet variable\n",
    "# The world has 3 rows and 4 columns\n",
    "env = GridWorld(3, 4)\n",
    "# Define the state matrix\n",
    "# Adding obstacle at position (1,1)\n",
    "# Adding the two terminal states\n",
    "state_matrix = np.zeros((3,4))\n",
    "state_matrix[0, 3] = 1\n",
    "state_matrix[1, 3] = 1\n",
    "state_matrix[1, 1] = -1\n",
    "# Define the reward matrix\n",
    "# The reward is -0.04 for all states but the terminal\n",
    "reward_matrix = np.full((3,4), -0.04)\n",
    "reward_matrix[0, 3] = 1\n",
    "reward_matrix[1, 3] = -1\n",
    "# Define the transition matrix\n",
    "# For each one of the four actions there is a probability\n",
    "transition_matrix = np.array([[0.8, 0.1, 0.0, 0.1],\n",
    "                              [0.1, 0.8, 0.1, 0.0],\n",
    "                              [0.0, 0.1, 0.8, 0.1],\n",
    "                              [0.1, 0.0, 0.1, 0.8]])\n",
    "# Define the policy matrix\n",
    "# 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT, NaN=Obstacle, -1=NoAction\n",
    "# This is the optimal policy for world with reward=-0.04\n",
    "policy_matrix = np.array([[1,      1,  1,  -1],\n",
    "                          [0, np.NaN,  0,  -1],\n",
    "                          [0,      3,  3,   3]])\n",
    "# Set the matrices \n",
    "env.setStateMatrix(state_matrix)\n",
    "env.setRewardMatrix(reward_matrix)\n",
    "env.setTransitionMatrix(transition_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
